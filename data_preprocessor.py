import re
import nltk
nltk.download('wordnet')


def preprocess_data(comments):
    stemmer = nltk.stem.WordNetLemmatizer()
    preprocessed_comments = []

    for index in range(0, len(comments)):
        # Converting to Lowercase
        comment = str(comments[index]).lower()
        # Removing prefixed '&gt;', '&lt;', '&amp', 'tl;dr', '/r/', '/u/', 'http', 'https', 'www', 'com',
        #                   'youtube.com/watch', 'youtu.be'
        # phrases generated by a bot => '*i am a bot, and this action was performed automatically...*'
        #                            => 'Very short discussion posts are usually a sign...'

        comment = re.sub(
            r'&gt;|&lt;|&amp;|tl;dr|https|http|www|\.com|np\.reddit|reddit|youtube\.com/watch|youtu\.be|/\S/|\*i am a bot(.+?)=/r/|\) if you have(.+?)\.\*|very short discussion((.|\n)*)the bot\.|This is ((.|\n)*)original]',
            ' ', comment)
        # Remove all the special characters
        comment = re.sub(r'\W', ' ', comment)
        # remove all single characters
        comment = re.sub(r'\s+[a-zA-Z]\s+', ' ', comment)
        # Remove single characters from the start
        comment = re.sub(r'\^[a-zA-Z]\s+', ' ', comment)
        # Substituting multiple spaces with single space
        comment = re.sub(r'\s+', ' ', comment, flags=re.I)
        # Lemmatization
        comment = comment.split()
        comment = [stemmer.lemmatize(word) for word in comment]
        comment = ' '.join(comment)
        preprocessed_comments.append(comment)

    return preprocessed_comments
